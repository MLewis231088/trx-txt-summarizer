{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Up5b7tEA_EEU","executionInfo":{"status":"ok","timestamp":1637226802916,"user_tz":-330,"elapsed":404,"user":{"displayName":"Malvica Lewis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoaG7ptzXZAzrtu6nh1S-1Pzb6YXagvhUM8CnOBDM=s64","userId":"18110126109176374143"}},"outputId":"d4cfff07-9ab1-46c7-81f9-2eb36aeeb80b"},"source":["#%%writefile app.py\n","\n","\n","#!pip install ipykernel #pip install --upgrade ipykernel\n","#!pip install pydeck #!pip install --upgrade pydeck\n","#!pip install --upgrade ipython #!pip install ipython\n","#!pip install transformers nlp datasets #imbalanced-learn\n","#!pip install streamlit\n","#!pip install streamlit-pandas-profiling\n","#!pip install --upgrade simpletransformers\n","\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n","\n","\n","import streamlit as st\n","import warnings\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-whitegrid')\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import plotly.figure_factory as ff\n","from scipy import stats\n","import itertools #Functions creating iterators for efficient looping\n","from PIL import Image #Python Imaging Library is an open-source library that provides support for opening, manipulating, and saving many different image file formats.\n","import plotly.express as px #Plotly Express provides more than 30 functions for creating different types of figures.\n","import pandas_profiling #pandas_profiling library in Python include a method named as ProfileReport() which generate a basic report on the input DataFrame. \n","from pathlib import Path\n","import logging\n","import re\n","import random\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","import warnings\n","import sklearn\n","from sklearn import metrics\n","import statistics # for calculating mode of the data\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n","from simpletransformers.classification import ClassificationModel\n","from streamlit_pandas_profiling import st_profile_report\n","from keras.models import load_model\n","from wordcloud import WordCloud\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Subtract, LSTM, Embedding, Bidirectional\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.initializers import Constant\n","from tensorflow.keras.models import Sequential, Model\n","import torch  #The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.\n","              #It has a CUDA counterpart, that enables you to run your tensor computations on GPU \n","import torch.nn as nn #Pytorch classes that helps to create and train Neural Networks. \n","from nlp import Dataset # Dataset stores the samples and their corresponding labels\n","from torch.utils.data import DataLoader #DataLoader wraps an iterable with specified batch size around the Dataset.\n","from sklearn import datasets #scikit-learn comes with a few small standard datasets that do not require to download any file from some external website\n","from datasets import load_metric # Used to load the metrics for calculating metrics\n","from transformers import BertTokenizerFast, AutoModelForSequenceClassification\n","#AutoModelForSequenceClassification model class will be instantiated as one of the model classes of the library (with a sequence classification head)\n","\n","from transformers import RobertaTokenizer,RobertaForSequenceClassification #RobertaConfig, RobertaModel, \n","pd.set_option('max_colwidth',200)\n","pd.set_option('max_columns',200)\n","import random\n","random.seed(100)\n","SEED = 100\n","\n","\n","PAGE_CONFIG = {\"page_title\":\"StColab.io\",\"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n","st.set_page_config(**PAGE_CONFIG)\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","st.set_option('deprecation.showPyplotGlobalUse', False)\n","\n","cover_image = Image.open('https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.newscientist.com%2Farticle%2F2275323-robot-taught-table-etiquette-can-explain-why-it-wont-follow-the-rules%2F&psig=AOvVaw0mSCHcsngmrrhDDQvVf42d&ust=1667488399593000&source=images&cd=vfe&ved=0CA0QjRxqFwoTCKCggMTkj_sCFQAAAAAdAAAAABAD')\n","st.image(cover_image, use_column_width=True) \n","\n","class DataFrame_Loader():    \n","    def __init__(self):        \n","        print(\"Loading the DataFrame file....\")\n","        \n","    def read_csv(self,data):\n","        self.df = pd.read_csv(data)\n","        return self.df\n","\n","    def save_df1(self,data,filename):\n","         path = \"/content/drive/MyDrive/Colab Notebooks/Capstone Project/Team Files\"\n","         filename = path+filename\n","         DF = pd.DataFrame(data)\n","         DF.to_csv(filename, encoding = 'utf-8', index=False)\n","         st.write(\"Saved at:\",filename)\n","\n","class EDA():\n","    def __init__(self):\n","        print(\"Performing Exploratory Data Analysis\")\n","\n","    def dis_shape(self,x):\n","        return x.shape\n","\n","    def dis_dtypes(self,x):\n","      import pyarrow as pa\n","      df_t = pd.DataFrame(columns=['Name','dtypes'])\n","      string = pa.Schema.from_pandas(x).to_string()\n","      for str1 in string.split('\\n'):\n","        str2 = str1.split(':',1)\n","        if len(str2) == 2 :\n","          df_t.loc[len(df_t.index)] = [str2[0],str2[1]]\n","      df_t.drop(df_t.tail(2).index,inplace=True)\n","      return df_t\n","\n","    def dis_missing(self,x):\n","    \t  return x.isna().sum()\n","\n","    def tabulation(self,x):\n","        table=pd.DataFrame(columns=['Columns','No. of missing values','% of missing values','No. of unique values','No. of non-zero values'])\n","        # table = self.dis_dtypes(x)\n","        table['Columns'] = x.columns\n","        table['No. of missing values'] = x.isnull().sum().values\n","        table['% of missing values'] = ((x.isnull().sum().values)/ (x.shape[0])) *100\n","        table['No. of unique values'] = x.nunique().values\n","        table['No. of non-zero values'] = np.count_nonzero(x, axis=0)\n","        return table\n","\n","    def dis_five_number_summary(self,x):\n","        return x.describe().transpose()\n","\n","    def dis_columns(self,x):\n","        return x.columns\n","\n","    def dis_categorical_variables(self,x):\n","        cat_var = [var for var in x.columns if x[var].dtypes==\"object\"]\n","        cat_var = x[cat_var]\n","        return cat_var\n","\n","    def dis_numerical_variables(self,x):\n","        num_var = [var for var in x.columns if x[var].dtypes!=\"object\"]\n","        num_var = x[num_var]\n","        return num_var\n","\n","    def dis_duplicates(self,x,*args):\n","        duplicateRowsDF = x[x.duplicated()]\n","        return (duplicateRowsDF)\n","\n","class visualize_data():\n","    def __init__(self):\n","        print(\"Visualizing data\")\n","\n","    def vis_displot(self,x):\n","        plt.style.use('fivethirtyeight')\n","        plt.figure(figsize=(20,10))\n","        return sns.distplot(x, bins = 25)\n","\n","    def vis_countplot(self,x,hue):\n","        plt.style.use('fivethirtyeight')\n","        plt.figure(figsize=(20,10))\n","        return sns.countplot(x, hue=hue, palette='summer')\n","\n","    def vis_barplot(self,x,y):\n","        plt.figure(figsize=(20,10))\n","        plt.xticks(rotation=90)\n","        plt.tight_layout()\n","        return sns.barplot(x, y, palette=\"ch:.25\")\n","\n","    def vis_piechart(self,x,col):\n","        colors = ['tab:blue', 'tab:cyan', 'tab:gray', 'tab:orange', 'tab:red']\n","        plt.figure(figsize=(10,5))\n","        return x[col].value_counts().plot(kind='pie', colors = colors, autopct='%.0f%%', shadow = True)\n","\n","    def vis_boxplot(self,x):\n","        plt.figure(figsize=(20,5))\n","        return sns.boxplot(x=x, palette='summer')\n","\n","    def vis_histplot(self,x):\n","    \treturn x.hist()\n","     \n","    def vis_pairplot(self,x):\n","        return sns.pairplot(x, palette='spring')\n","\n","    def vis_heatmap(self,x):\n","        plt.figure(figsize=(20,20))\n","        return sns.heatmap(x.corr(),annot=True,cmap=\"YlGnBu\");\n","\n","    def vis_scatterplot(self,x,col1,col2):\n","        rng = np.random.RandomState(0)\n","        colors = rng.rand(x.shape[0])\n","        return plt.scatter(col1,col2,c=colors,cmap='viridis')      \n","\n","    def vis_fulldisplot(self,x):\n","        color = sns.color_palette(\"viridis\", 20)\n","        fig, ax = plt.subplots(figsize = (20, 65))\n","        for n, col in enumerate(x.columns):\n","          try:\n","            if (x[col].dtype.name == 'int64' or x[col].dtype.name == 'float64'):\n","              if x.shape[1]%2==0:\n","                plt.subplot((x.shape[1]+1)/2, 2, n)\n","                sns.distplot(x[col], kde = True, color=color[n-3])\n","                plt.title(f\"\\nDistribution of {col}\\n\",fontdict=dict(fontsize=20))\n","                plt.tight_layout();\n","              else:\n","                plt.subplot((x.shape[1]+1)/2, 2, n+1)\n","                sns.distplot(x[col], kde = True, color=color[n-3])\n","                plt.title(f\"\\nDistribution of {col}\\n\",fontdict=dict(fontsize=20))\n","                plt.tight_layout();               \n","            else:\n","              print(f\"\\n Distplot is unavailable for categorical variable '{col}'\")\n","              pass \n","          except IndexError:\n","              pass\n","\n","    def vis_fullboxplot(self,x):\n","        color = sns.cubehelix_palette(20)\n","        fig, ax = plt.subplots(figsize = (20, 65))\n","        for n, col in enumerate(x.columns):\n","          try:\n","            if (x[col].dtype.name == 'int64' or x[col].dtype.name == 'float64'):\n","              if x.shape[1]%2==0:\n","                plt.subplot((x.shape[1]+1)/2, 2, n)\n","                sns.boxplot(x[col], color=color[n-3])\n","                plt.title(f\"\\nDistribution of {col}\\n\",fontdict=dict(fontsize=20))\n","                plt.tight_layout();\n","              else:\n","                plt.subplot((x.shape[1]+1)/2, 2, n+1)\n","                sns.boxplot(x[col], color=color[n-3])\n","                plt.title(f\"\\nDistribution of {col}\\n\",fontdict=dict(fontsize=20))\n","                plt.tight_layout();               \n","            else:\n","              print(f\"\\n Boxplot is unavailable for categorical variable '{col}'\")\n","              pass \n","          except IndexError:\n","              pass\n","\n","    def vis_wordcloud(self,x):\n","\t    wordcloud = WordCloud(width = 3000, height = 2000).generate(\" \".join(x))\n","\t    plt.imshow(wordcloud);\n","\t    plt.axis(\"off\")\n","\t    return wordcloud  \n","\n","class preprocess_data():\n","    def __init__(self):\n","        print(\"Data Preprocessing : \")\n","\n","    def drop_Column(self,x,col_name):\n","      st.write(col_name)\n","      return x.drop(columns=col_name)\n","\n","    def drop_rows_withoutApt_data(self,x):\n","      return x.dropna(axis=0)\n","    \n","    def remove_stopword(self,x):\n","      import nltk #NLP Tool Kit\n","      from nltk.corpus import stopwords\n","      nltk.download('stopwords')\n","      stopwords=set(stopwords.words('english'))\n","      return x.apply(lambda s: ' '.join([words for words in s.split() if words not in stopwords]))\n","    \n","    def tokenize(self,x):\n","      from keras.preprocessing.text import Tokenizer\n","      vocab_size = 10000\n","      oov_token = \"<OOV>\"\n","      tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n","      tokenizer.fit_on_texts(x)\n","      return tokenizer.texts_to_sequences(x)\n","\n","    def select_feature(self,x,columnName):\n","      return x[columnName]\n","    \n","    def select_label(self,y,columnName):\n","      return y[columnName]\n","    \n","    def padding(self,x,length,paddingType,truncationType):\n","      from keras.preprocessing.sequence import pad_sequences\n","      return pad_sequences(x,length,padding=paddingType,truncating=truncationType)\n","\n","    def label_classification(self,x):\n","      from sklearn.preprocessing import OneHotEncoder\n","      oneHotEnc = OneHotEncoder()\n","      return oneHotEnc.fit_transform(x.values.reshape(-1,1)).toarray()\n","    \n","    def train_test_split(self,feature,label):\n","      from sklearn.model_selection import train_test_split\n","      X_train, X_test, y_train, y_test = train_test_split(feature, label, test_size=0.3, random_state=42)\n","      X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3, random_state=42)\n","      return X_train,y_train,X_test,y_test,X_val,y_val\n","\n","    def print_Len_details(self,x):\n","      X_sequences = x\n","      lenData = len(X_sequences)\n","      maxWordCount = 0\n","      minWordCount = len(X_sequences[0])\n","      avgWordCount = 0\n","      totalWordCount = 0\n","      wordCount_list = []\n","      for i in range(lenData):\n","        wordCount = len(X_sequences[i])\n","        wordCount_list.append(wordCount)\n","        avgWordCount = (wordCount+totalWordCount) / (i+1)\n","        totalWordCount = totalWordCount + wordCount\n","        if wordCount > maxWordCount :\n","            maxWordCount = wordCount\n","        \n","        if wordCount < minWordCount :\n","            minWordCount = wordCount\n","\n","      st.write(\"Maximum Word Limit in a Review : \",maxWordCount)\n","      st.write(\"Minimum Word Limit in a Review : \",minWordCount)\n","      st.write(\"Average Word Limit in a Review across all Reviews : \",avgWordCount)\n","\n","class models():\n","    def __init__(self):\n","        print(\"Building models\")\n","\n","    def fsl_1inp(self, df_train, df_test):\n","        labels = df_train['Accident Level'].unique()\n","        text_left = []\n","        text_right = []\n","        target = []\n","\n","        for label in labels:\n","            \n","            similar_texts = df_train[df_train['Accident Level']==label]['cleaned_text']\n","            group_similar_texts = list(itertools.combinations(similar_texts,2))\n","            \n","            text_left.extend([group[0] for group in group_similar_texts])\n","            text_right.extend([group[1] for group in group_similar_texts])\n","            target.extend([1.]*len(group_similar_texts))\n","\n","            dissimilar_texts = df_train[df_train['Accident Level']!=label]['cleaned_text']\n","            for i in range(len(group_similar_texts)):\n","                text_left.append(np.random.choice(similar_texts))\n","                text_right.append(np.random.choice(dissimilar_texts))\n","                target.append(0.)\n","                \n","        dataset = pd.DataFrame({'text_left':text_left,\n","                            'text_right':text_right,\n","                            'target': target})\n","        st.write('The Siamese Dataframe is created.')\n","        st.dataframe(dataset.head())\n","        MAX_SEQ_LENGTH = 200\n","        VOCAB_SIZE = 10000\n","\n","        tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n","        tokenizer.fit_on_texts(df_train.cleaned_text)\n","        sequences_left = tokenizer.texts_to_sequences(dataset.text_left)\n","        sequences_right = tokenizer.texts_to_sequences(dataset.text_right)\n","\n","        word_index = tokenizer.word_index\n","\n","        x_left = pad_sequences(sequences_left, maxlen=MAX_SEQ_LENGTH)\n","        x_right = pad_sequences(sequences_right, maxlen=MAX_SEQ_LENGTH)\n","\n","\n","        # load model in Keras\n","        siamese_lstm = load_model('/content/drive/MyDrive/Colab Notebooks/Capstone Project/Models/fsl_model_1input.h5')\n","\n","        reference_sequences = tokenizer.texts_to_sequences(df_train.cleaned_text)\n","        x_reference_sequences = pad_sequences(reference_sequences, maxlen=MAX_SEQ_LENGTH)\n","\n","        def flatten_text_sequence(text):\n","            flatten = itertools.chain.from_iterable\n","            text = list(flatten(text))\n","            return text\n","\n","        def get_prediction(text):\n","            \"\"\" Get the predicted category, and the most similar text\n","            in the train set. Note that this way of computing a prediction is highly \n","            not optimal, but it'll be sufficient for us now. \"\"\"\n","            x = tokenizer.texts_to_sequences(text.split())\n","            x = flatten_text_sequence(x)\n","            x = pad_sequences([x], maxlen=MAX_SEQ_LENGTH)\n","\n","            # Compute similarities of the text with all text's in the train set\n","            result = np.repeat(x, len(x_reference_sequences), axis=0)\n","            similarities = siamese_lstm.predict([result, x_reference_sequences])\n","            most_similar_index = np.argmax(similarities)\n","            \n","            # The predicted category is the one of the most similar example from the train set\n","            prediction = df_train['Accident Level'].iloc[most_similar_index]\n","            most_similar_example = df_train['cleaned_text'].iloc[most_similar_index]\n","\n","            return prediction, most_similar_example\n","\n","        classes_encoder = LabelEncoder()\n","\n","        y_train = classes_encoder.fit_transform(df_train['Accident Level'])\n","        y_test = classes_encoder.transform(df_test['Accident Level'])\n","\n","        y_pred = [get_prediction(Description)[0] for Description in df_test['Description']]\n","        accuracy = accuracy_score(classes_encoder.transform(y_pred), y_test)\n","\n","        target_names = ['Acc Level 1', 'Acc Level 2', 'Acc Level 3', 'Acc Level 4', 'Acc Level 5']\n","        cm = confusion_matrix(classes_encoder.transform(y_pred), y_test)\n","        # print(classification_report(classes_encoder.transform(y_pred), y_test, target_names=target_names))\n","\n","        def plot_confusion_matrix(cm, classes,\n","                                normalize=False,\n","                                title='Confusion matrix',\n","                                cmap=plt.cm.Purples):\n","            \"\"\"\n","            This function prints and plots the confusion matrix.\n","            Normalization can be applied by setting `normalize=True`.\n","            \"\"\"\n","            plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","            plt.title(title)\n","            plt.colorbar()\n","            tick_marks = np.arange(len(classes))\n","            plt.xticks(tick_marks, classes, rotation=45)\n","            plt.yticks(tick_marks, classes)\n","\n","            if normalize:\n","                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","                print(\"Normalized confusion matrix\")\n","            else:\n","                print('Confusion matrix, without normalization')\n","\n","            thresh = cm.max() / 2.\n","            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","                plt.text(j, i, cm[i, j],\n","                    horizontalalignment=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","            plt.tight_layout()\n","            plt.ylabel('True label')\n","            plt.xlabel('Predicted label')\n","\n","        precision = metrics.precision_score(y_test, classes_encoder.transform(y_pred),average='macro')\n","        recall = metrics.recall_score(y_test, classes_encoder.transform(y_pred), average='micro')\n","        f1 = metrics.f1_score(y_test, classes_encoder.transform(y_pred), average='weighted')\n","        model_performance = pd.DataFrame(columns=['Model', 'Accuracy', 'Log Loss','Precision', 'Recall', 'F1'])\n","        model_performance = model_performance.append({'Model':'FSL + BiLSTM + Data Aug (Syn replacement) for class II, III, IV and V',\n","                                      'Accuracy': accuracy,\n","                                      'Log Loss': 'NA',\n","                                      'Precision': precision,\n","                                      'Recall': recall,\n","                                      'F1': f1                                    \n","                                      }, ignore_index=True)\n","\n","\n","        return (f'Test accuracy: {100*accuracy:.2f} %', \n","                plot_confusion_matrix(cm=cm, classes=target_names, title='Confusion Matrix for Few-Shot Learning Model with 1 feature'),\n","                classification_report(classes_encoder.transform(y_pred), y_test, target_names=target_names),\n","                model_performance\n","                )\n","\n","    def xlnet(self,df_test):\n","        model_type = 'xlnet'\n","        model_name = 'xlnet-base-cased'\n","\n","        model = ClassificationModel(model_type, \"/content/drive/MyDrive/Colab Notebooks/Capstone Project/outputs/xlnet/final\", use_cuda=True,)\n","        y_preds, _, = model.predict(df_test['text'])\n","\n","\n","        def compute_metrics(y_true, y_pred):\n","            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n","            return {\n","                'f1': f1,\n","                'precision': precision,\n","                'recall': recall\n","            }\n","\n","        # Evaluate the model\n","        result, model_outputs, wrong_predictions = model.eval_model(df_test, \n","                                                                    acc = sklearn.metrics.accuracy_score\n","                                                                    )\n","        \n","        def plot_cm(y_true, y_pred, title, figsize=(5,5)):\n","            cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n","            cm_sum = np.sum(cm, axis=1, keepdims=True)\n","            cm_perc = cm / cm_sum.astype(float) * 100\n","            annot = np.empty_like(cm).astype(str)\n","            nrows, ncols = cm.shape\n","            for i in range(nrows):\n","                for j in range(ncols):\n","                    c = cm[i, j]\n","                    p = cm_perc[i, j]\n","                    if i == j:\n","                        s = cm_sum[i]\n","                        annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n","                    elif c == 0:\n","                        annot[i, j] = ''\n","                    else:\n","                        annot[i, j] = '%.1f%%\\n%d' % (p, c)\n","            cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n","            cm.index.name = 'Actual'\n","            cm.columns.name = 'Predicted'\n","            fig, ax = plt.subplots(figsize=figsize)\n","            plt.title(title)\n","            sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n","        \n","        return (result, \n","                plot_cm(y_preds, df_test['label'].values, 'Confusion matrix for XLNet model', figsize=(7,7)),\n","                compute_metrics(df_test['label'], y_preds))\n","\n","    def distilbert(self,df_test):\n","        model_type = 'distilbert'\n","        model_name = 'distilbert-base-cased'\n","\n","        model = ClassificationModel(model_type, \"/content/drive/MyDrive/Colab Notebooks/Capstone Project/outputs/distilbert/final\", use_cuda=True,)\n","\n","        def compute_metrics(y_true, y_pred):\n","            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n","            return {\n","                'f1': f1,\n","                'precision': precision,\n","                'recall': recall\n","            }\t\n","\n","        # Evaluate the model\n","        result, model_outputs, wrong_predictions = model.eval_model(df_test, \n","                                                                    acc = sklearn.metrics.accuracy_score\n","                                                                    )\n","        \n","        def plot_cm(y_true, y_pred, title, figsize=(5,5)):\n","            cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n","            cm_sum = np.sum(cm, axis=1, keepdims=True)\n","            cm_perc = cm / cm_sum.astype(float) * 100\n","            annot = np.empty_like(cm).astype(str)\n","            nrows, ncols = cm.shape\n","            for i in range(nrows):\n","                for j in range(ncols):\n","                    c = cm[i, j]\n","                    p = cm_perc[i, j]\n","                    if i == j:\n","                        s = cm_sum[i]\n","                        annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n","                    elif c == 0:\n","                        annot[i, j] = ''\n","                    else:\n","                        annot[i, j] = '%.1f%%\\n%d' % (p, c)\n","            cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n","            cm.index.name = 'Actual'\n","            cm.columns.name = 'Predicted'\n","            fig, ax = plt.subplots(figsize=figsize)\n","            plt.title(title)\n","            sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n","\n","        y_preds, _, = model.predict(df_test['text'])\n","\n","        \n","        return (result, \n","                plot_cm(y_preds, df_test['label'].values, 'Confusion matrix for DistilBert model', figsize=(7,7)),\n","                compute_metrics(df_test['label'], y_preds))\n","\n","class modeldisplay_functions():\n","    def __init__(self):        \n","        print(\"Loading BERT models\")\n","        self.__bert_df__ = pd.DataFrame()\n","        \n","\n","    def data_preprocess01(self,df): # Function to create a DataFrame with 2 columns \"Accident Level\" and \"Description\"\n","      self.__bert_df__ = df     \n","      self.__bert_df__ = self.__bert_df__.drop(['Unnamed: 0','Data','Countries','Local','Industry Sector','Potential Accident Level','Genre','Employee or Third Party','Critical Risk'], axis = 1) # Drop of Attributes\n","      categorical_to_numeric = {\"Accident Level\": {\"I\": 0, \"II\": 1,\"III\": 2, \"IV\": 3,\"V\": 4}}\n","      self.__bert_df__ = self.__bert_df__.replace(categorical_to_numeric)\n","      return self.__bert_df__ \n","    \n","    def bert_preprocess(self,df):   # function converts the text to tokens and adds padding to the tokens of maximum length 200\n","      self.bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\n","      return self.bert_tokenizer(df['text'], truncation=True, max_length = 200, padding='max_length')\n","\n","    def roberta_preprocess(self,df):   # function converts the text to tokens and adds padding to the tokens of maximum length 200\n","      self.roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\",do_lower_case=True)\n","      return self.roberta_tokenizer(df['text'], truncation=True, max_length = 200, padding='max_length')\n","\n","\n","    def bert_prediction_display(self,text, model): \n","      text_list = []\n","      dataset = {}\n","      pred_list = []\n","      prediction = 6\n","\n","      pred_length = 3 # creating same 'text' 3 times for batch purpose. This can be varied the minimum is 2. \n","      for i in range(pred_length): \n","        text_list.append(text)  \n","\n","      dataset = dict([('text', text_list)]) # converts the data to dict format\n","      dataset = Dataset.from_dict(dataset) # converts the data to Dataset format\n","      dataset_batch = dataset.map(self.bert_preprocess, batched=True,batch_size=len(dataset)) #converts the text to tokens\n","      dataset_batch.set_format('torch', columns=['input_ids', 'attention_mask'])\n","      dataset_dataloader = DataLoader(dataset_batch, shuffle=True, batch_size=64) #converts the data into DataLoader format with a batch size of 64\n","\n","      dev = torch.device(\"cpu\") #if torch.cuda.is_available() else torch.device(\"cpu\")\n","      device_pickle = torch.device(dev) # functions that utilize GPU's for computation purpose. \n","      model.to(device_pickle)\n","\n","      model.eval()\n","      for batch in dataset_dataloader:\n","        batch = {k: v.to(device_pickle) for k, v in batch.items()}\n","        with torch.no_grad():\n","          outputs = model(**batch)\n","\n","      logits = outputs.logits\n","    \n","      predictions = torch.argmax(logits, dim=-1)\n","      pred_list = predictions.tolist()\n","      try:\n","        prediction = statistics.mode(pred_list)\n","      except:\n","        prediction = pred_list[0]\n","      #print(\"Predictions:{}\".format(prediction))\n","      return prediction\n","\n","    def roberta_prediction_display(self,text,model): \n","      text_list = []\n","      dataset = {}\n","      pred_list = []\n","      prediction = 6\n","\n","      pred_length = 3 # creating same 'text' 3 times for batch purpose. This can be varied the minimum is 2. \n","      for i in range(pred_length): \n","        text_list.append(text)  \n","\n","      dataset = dict([('text', text_list)]) # converts the data to dict format\n","      dataset = Dataset.from_dict(dataset) # converts the data to Dataset format\n","      dataset_batch = dataset.map(self.bert_preprocess, batched=True,batch_size=len(dataset)) #converts the text to tokens\n","      dataset_batch.set_format('torch', columns=['input_ids', 'attention_mask'])\n","      dataset_dataloader = DataLoader(dataset_batch, shuffle=True, batch_size=64) #converts the data into DataLoader format with a batch size of 64\n","\n","      dev = torch.device(\"cpu\") #if torch.cuda.is_available() else torch.device(\"cpu\")\n","      device_roberta = torch.device(dev) # functions that utilize GPU's for computation purpose. \n","      model.to(device_roberta)\n","\n","      model.eval()\n","      for batch in dataset_dataloader:\n","        batch = {k: v.to(device_roberta) for k, v in batch.items()}\n","        with torch.no_grad():\n","          outputs = model(**batch)\n","\n","      logits = outputs.logits\n","    \n","      predictions = torch.argmax(logits, dim=-1)\n","      pred_list = predictions.tolist()\n","      try:\n","        prediction = statistics.mode(pred_list)\n","      except:\n","        prediction = pred_list[0]\n","      #print(\"Predictions:{}\".format(prediction))\n","      return prediction         \n","\n","def main():\n","  st.title(\"NLP Chatbot Interface\")\n","  st.info(\"Capstone Project_Group 11_NLP 2_August 20 Batch\")\n","  \n","  activities = [\"Perform Exploratory Data Analysis\",\"Visualize Data\",\"Preprocess Data\",\"Build Models for NLP Tasks\"]\n","  task = st.sidebar.selectbox(\"Please select your preferred task\",activities)\n","  \n","  isDataLoadingReq = True\n","\n","  if task == 'Perform Exploratory Data Analysis':\n","    st.subheader(\"Exploratory Data Analysis\")\n","    data = st.file_uploader(\"Please upload a Dataset\", type=[\"csv\"])\n","    if data is not None:\n","      df = load.read_csv(data)\n","      df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n","      st.dataframe(df.head())\n","      st.success(\"The dataset is loaded successfully!\")\n","\n","      if st.checkbox(\"Display the shape of the dataset\"):\n","          st.write(dataframe.dis_shape(df))\n","\n","      if st.checkbox(\"Display the object type of attributes\"):\n","          st.write(dataframe.dis_dtypes(df))\n","\n","      if st.checkbox(\"Display missing entries in the dataset\"):\n","          st.write(dataframe.dis_missing(df))\n","\n","      if st.checkbox(\"Display missing values; unique values; % of missing values; first, second and third observations; and entropy\"):\n","          st.write(dataframe.tabulation(df))\n","\n","      if st.checkbox(\"Display five-number summary of numerical attributes\"):\n","          st.write(dataframe.dis_five_number_summary(df))\n","\n","      if st.checkbox(\"Display categorical variables\"):\n","          new_df = dataframe.dis_categorical_variables(df)\n","          catego_df = pd.DataFrame(new_df)\n","          st.dataframe(catego_df)\n","\n","      if st.checkbox(\"Display numerical variables\"):\n","          num_df = dataframe.dis_numerical_variables(df)\n","          numer_df=pd.DataFrame(num_df)\n","          st.dataframe(numer_df)\n","\n","      if st.checkbox(\"Display value counts of categories in categorical variables\"):\n","          cat_vars = [var for var in df.columns if df[var].dtypes==\"object\"]\n","          sel_cat_var = st.selectbox(\"Select the categorical variable\",cat_vars)\n","          # col = pd.DataFrame(df[sel_cat_var].values)\n","          st.write(df[sel_cat_var].value_counts())\n","\n","      if st.checkbox(\"Display skewness of the dataset\"):\n","          st.write(df.skew(axis = 0, skipna = True))\n","\n","      if st.checkbox(\"Display duplicate rows in the dataset\"):\n","          all_cols = dataframe.dis_columns(df)\n","          data1 = dataframe.dis_duplicates(df)\n","          st.write(data1)\n","          st.write(\"The number of duplicates are: \", len(data1))\n","\n","  elif task == 'Visualize Data':\n","    st.subheader(\"Visualizing the distribution of data\")\n","    data = st.file_uploader(\"Please upload a Dataset\", type=[\"csv\"])\n","    if data is not None:\n","      df = load.read_csv(data)\n","      df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n","      st.dataframe(df.head())\n","      st.success(\"The dataset is loaded successfully!\")\n","\n","      st.subheader('Univariate Analysis')\n","\n","      if st.checkbox(\"Display DISTPLOT of numerical attributes\"):\n","          num_vars = [var for var in df.columns if df[var].dtypes!=\"object\"]\n","          sel_num_var = st.selectbox(\"Select the numerical variable\",num_vars, key=\"num_vars1\")\n","          if st.button(\"Generate distplot for the selected variable\"):\n","            st.write(visualize.vis_displot(df[sel_num_var]))\n","            st.pyplot()\n","\n","      if st.checkbox(\"Display COUNTPLOT of categorical attributes\"):\n","          cat_vars = [var for var in df.columns if df[var].dtypes==\"object\"]\n","          sel_cat_var = st.selectbox(\"Select the categorical variable\",cat_vars)\n","          hue = st.selectbox(\"Select the hue attribute\",cat_vars, key=\"cat_vars1\")\n","          if st.button(\"Generate countplot for the selected variable\"):\n","            st.write(visualize.vis_countplot(df[sel_cat_var],df[hue]))\n","            st.pyplot()\n","\n","      if st.checkbox(\"Display PIECHART of categorical attributes\"):\n","          cat_vars = [var for var in df.columns if df[var].dtypes==\"object\"]\n","          sel_cat_var1 = st.selectbox(\"Select the categorical variable\",cat_vars, key=\"cat_vars2\")\n","          if st.button(\"Generate piechart for the selected variable\"):\n","            st.write(visualize.vis_piechart(df,sel_cat_var1))\n","            st.pyplot()\n","\n","      if st.checkbox(\"Display outliers using BOXPLOTS\"):\n","          cols = dataframe.dis_columns(df) \n","          sel_x = st.selectbox(\"Select the variable\",cols, key=\"cols3\")\n","          if st.button(\"Generate boxplot for the selected variable\"):\n","            st.write(visualize.vis_boxplot(df[sel_x]))\n","            st.pyplot()\n","      \n","      if st.checkbox(\"Display HISTOGRAM of attributes\"):\n","          cols = dataframe.dis_columns(df) \n","          sel_col = st.selectbox(\"Select the variable\",cols, key=\"cols4\")\n","          if st.button(\"Generate histogram for the selected variable\"):\n","            st.write(visualize.vis_histplot(df[sel_col]))\n","            st.pyplot()\n","\n","      st.subheader('Bivariate Analysis')\n","\n","      if st.checkbox(\"Display SCATTERPLOT of numerical attributes\"):\n","          num_vars = [var for var in df.columns if df[var].dtypes!=\"object\"]\n","          sel_num_var1 = st.selectbox(\"Select the first numerical variable\",num_vars, key=\"num_vars2\")\n","          sel_num_var2 = st.selectbox(\"Select the second numerical variable\",num_vars, key=\"num_vars3\")\n","          if st.button(\"Generate scatterplot for the selected variables\"):\n","            st.write(visualize.vis_scatterplot(df,df[sel_num_var1],df[sel_num_var2]))\n","            st.pyplot()\n","\n","      if st.checkbox(\"Display BARPLOT  of attributes\"):\n","          cols = dataframe.dis_columns(df) \n","          sel_x = st.selectbox(\"Select the x variable\",cols, key=\"cols1\")\n","          sel_y = st.selectbox(\"Select the y variable\",cols, key=\"cols2\")\n","          if st.button(\"Generate barplot for the selected variables\"):\n","            st.write(visualize.vis_barplot(df[sel_x],df[sel_y]))\n","            st.pyplot()\n","\n","      st.subheader(\"Multivariate Analysis\")\n","\n","      if st.checkbox(\"Display PAIRPLOT of numerical attributes\"):\n","          st.write(visualize.vis_pairplot(df))\n","          st.pyplot()\n","\n","      if st.checkbox(\"Display HEATMAP to view the correlation between numerical attributes\"):\n","          st.write(visualize.vis_heatmap(df))\n","          st.pyplot()\n","\n","      if st.checkbox(\"Display DISTPLOT of the entire dataset\"):\n","          st.write(visualize.vis_fulldisplot(df))\n","          st.pyplot()\n","\n","      if st.checkbox(\"Display BOXPLOT of the entire dataset\"):\n","          st.write(visualize.vis_fullboxplot(df))\n","          st.pyplot()\n","\n","      # if st.checkbox(\"Display the complete data PROFILE REPORT\"):\n","      #     pr = df.profile_report()\n","      #     st_profile_report(pr)\n","\n","      if st.checkbox(\"Display WORD CLOUD\"):\n","          cols = dataframe.dis_columns(df) \n","          sel_col = st.selectbox(\"Select the attribute\",cols, key=\"cols10\")\n","          if st.button(\"Generate word cloud for the selected attribute\"):        \n","            st.write(visualize.vis_wordcloud(df[sel_col]))\n","            st.pyplot()\n","\n","  elif task == 'Preprocess Data':\n","    st.subheader(\"Pre-processing data to get clean data for modelling\")\n","    data = st.file_uploader(\"Please upload a Dataset\", type=[\"csv\"])\n","    if data is not None:\n","      if isDataLoadingReq == True :\n","        df = load.read_csv(data)\n","        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n","        df_EDA = df.copy(deep = True)\n","        st.write(\"Reloading data !!!!! \")\n","        isDataLoadingReq = False\n","        st.subheader(\"Pre-processing based on EDA Analysis\")\n","        st.write(\"Dropping the unnecessary columns\")\n","        column_name = [var for var in df.columns]\n","        column_to_keep = ['Accident Level', 'Description']\n","        column_to_drop = ['Data', 'Countries', 'Local', 'Industry Sector', \\\n","                          'Potential Accident Level', 'Genre', \\\n","                          'Employee or Third Party', 'Critical Risk']\n","        st.write(\"Based on EDA, we will drop the following Columns\")\n","        st.write(column_to_drop)\n","        st.write(\"Based on EDA, we will retain the following Columns\")\n","        st.write(column_to_keep)\n","        df_EDA = df_EDA.drop(columns=column_to_drop)\n","        st.write(df_EDA)\n","        st.write(dataframe.dis_shape(df_EDA))\n","\n","        st.subheader(\"Selecting Features and Labels for Model Input\")\n","        features = df_EDA['Description']\n","        labels = df_EDA['Accident Level']\n","        st.write(\"Features\",features)\n","        st.write(\"Labels\",labels)\n","\n","        st.subheader(\"Removing Stopwords: \")\n","        features = preprocess.remove_stopword(features)\n","        st.write(\"Features after StopWord Removal\",features)\n","        \n","        st.subheader(\"Tokenizing:\")\n","        features = preprocess.tokenize(features)\n","        preprocess.print_Len_details(features)\n","        st.write(\"Features after Tokenization\",features)\n","\n","        st.subheader(\"Padding Sequences:\")\n","        max_length = 200\n","        padding_type='post'\n","        truncation_type='post'  \n","        features = preprocess.padding(features,max_length,padding_type,truncation_type)\n","        preprocess.print_Len_details(features)\n","        st.write(\"Features after Padding\",features)\n","\n","        st.subheader(\"Label Class Identification:\")\n","        labels_enc = preprocess.label_classification(labels)\n","        st.write(labels_enc)\n","\n","        st.subheader(\"Splitting data for Model : train / validation / test split\")\n","        X_train,y_train,X_test,y_test,X_val,y_val = preprocess.train_test_split(features,labels_enc)\n","        st.write(\"Training data shape = \",X_train.shape)\n","        st.write(\"Test data shape = \",X_test.shape)\n","        st.write(\"Validation data shape = \",X_val.shape)\n","\n","        st.subheader(\"Saving file for the Model:\")\n","        load.save_df1(X_train,\"X_train.csv\")\n","        load.save_df1(y_train,\"y_train.csv\")\n","        load.save_df1(X_test,'X_test.csv')\n","        load.save_df1(y_test,'y_test.csv')\n","        load.save_df1(X_val,'X_val.csv')\n","        load.save_df1(y_val,'y_val.csv')\n","\n","  elif task == 'Build Models for NLP Tasks':\n","    st.subheader(\"Building Models for NLP Tasks\")\n","    data = st.file_uploader(\"Please upload a Dataset\", type=[\"csv\"])\n","    if data is not None:\n","      df = load.read_csv(data)\n","      st.dataframe(df.head())\n","      st.success(\"The dataset is loaded successfully!\")\n","\n","    if st.checkbox(\"Display of preprocessed data\"):\n","        #df = load.read_csv(data)\n","        preprocess_df = bm_func.data_preprocess01(df)\n","        st.write(preprocess_df.head())\n","\n","    st.subheader('Please select a model:')\n","\n","    if st.checkbox(\"XLNet Model\"):\n","        # data_train_xl = st.file_uploader(\"Please upload the Dataset\", type=[\"csv\"], key=\"k0\")\n","        # if data_train_xl is not None:\n","        #   df_train_xl = load.read_csv(data_train_xl)\n","        #   st.dataframe(df_train_xl.head())\n","        #   st.success(\"The dataset is loaded successfully!\")\n","        data_test_xl = st.file_uploader(\"Please upload the Test Dataset\", type=[\"csv\"], key=\"k1\")\n","        if data_test_xl is not None:\n","          df_test_xl = load.read_csv(data_test_xl)\n","          st.dataframe(df_test_xl.head())\n","          st.success(\"The dataset is loaded successfully!\")\n","          st.write(models.xlnet(df_test_xl))\n","          st.pyplot()  \n","\n","    if st.checkbox(\"DistilBert Model\"):\n","        # data_train_db = st.file_uploader(\"Please upload the Dataset\", type=[\"csv\"], key=\"k2\")\n","        # if data_train_db is not None:\n","        #   df_train_db = load.read_csv(data_train_db)\n","        #   st.dataframe(df_train_db.head())\n","        #   st.success(\"The dataset is loaded successfully!\")\n","        data_test_db = st.file_uploader(\"Please upload the Test Dataset\", type=[\"csv\"], key=\"k3\")\n","        if data_test_db is not None:\n","          df_test_db = load.read_csv(data_test_db)\n","          st.dataframe(df_test_db.head())\n","          st.success(\"The dataset is loaded successfully!\")\n","          st.write(models.distilbert(df_test_db))\n","          st.pyplot()     \n","        \n","    if st.checkbox(\"BERT Model Prediction\"):\n","      with st.form(key='my_form1'):\n","        text_input1 = st.text_area(\"Please enter the accident description within 200 characters:\",max_chars = 500)\n","        submit_button1 = st.form_submit_button(label='Predict the Accident Level')\n","      \n","      if submit_button1:          \n","        #tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)  \n","        pickledmodel_bert = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n","        pickledmodel_bert.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/Capstone Project/Team Files/Srinath/model_BERT.pth\"))\n","        #st.write(torch.cuda.is_available())\n","        \n","        prediction_bert = bm_func.bert_prediction_display(text_input1,pickledmodel_bert) \n","        AccidentLevel = ['I','II','III','IV','V'] # to display the text\n","        st.success(\"Predicted Accident Level based on the Description:\")\n","        st.write(AccidentLevel[prediction_bert])  \n","\n","    if st.checkbox(\"RoBERTa Model Prediction\"):\n","      with st.form(key='my_form2'):\n","        text_input2 = st.text_area(\"Please enter the accident description within 200 characters:\",max_chars = 500)\n","        submit_button2 = st.form_submit_button(label='Predict the Accident Level')\n","      \n","      if submit_button2:          \n","        \n","        pickledmodel_Roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n","        pickledmodel_Roberta.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/Capstone Project/Team Files/Srinath/model_Roberta1.pth\"))\n","        #st.write(torch.cuda.is_available())\n","        \n","        prediction_roberta = bm_func.roberta_prediction_display(text_input2,pickledmodel_Roberta) \n","        AccidentLevel = ['I','II','III','IV','V'] # to display the text\n","        st.success(\"Predicted Accident Level based on the Description:\")\n","        st.write(AccidentLevel[prediction_roberta])   \n","\n","\n","    if st.checkbox(\"Few-Shot Learning Model with BiLSTM\"):\n","        data_train_fsl = st.file_uploader(\"Please upload the Train Dataset to create the Siamese Dataframe\", type=[\"csv\"], key=\"k4\")\n","        if data_train_fsl is not None:\n","          df_train_fsl = load.read_csv(data_train_fsl)\n","          st.dataframe(df_train_fsl.head())\n","          st.success(\"The dataset is loaded successfully!\")\n","        data_test_fsl = st.file_uploader(\"Please upload the Test Dataset\", type=[\"csv\"], key=\"k5\")\n","        if data_test_fsl is not None:\n","          df_test_fsl = load.read_csv(data_test_fsl)\n","          st.dataframe(df_test_fsl.head())\n","          st.success(\"The dataset is loaded successfully!\")\n","          st.write(models.fsl_1inp(df_train_fsl, df_test_fsl))\n","          st.pyplot()\n","\n","\n","\n","      \n","if __name__ == '__main__':\n","    load = DataFrame_Loader()\n","    dataframe = EDA()\n","    visualize = visualize_data()\n","    preprocess = preprocess_data()\n","    bm_func = modeldisplay_functions()\n","    models = models()\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","metadata":{"id":"CD2YJioiiEIZ"},"source":[],"execution_count":null,"outputs":[]}]}