# -*- coding: utf-8 -*-
"""Copy of GUI_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10d4vMlfltkFXIcC913ojEOf8jcWLrGUI
"""

#%%writefile app.py
python -m pip install ipykernel #pip install --upgrade ipykernel
python -m pip install pydeck #!pip install --upgrade pydeck
python -m pip install --upgrade ipython #!pip install ipython
python -m pip install transformers nlp datasets #imbalanced-learn
python -m pip install streamlit
python -m pip install streamlit-pandas-profiling
python -m pip install --upgrade simpletransformers

python -m pip install google.colab

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1" 


import streamlit as st
import warnings
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import matplotlib
matplotlib.use("Agg")
import plotly.figure_factory as ff
from scipy import stats
import itertools #Functions creating iterators for efficient looping
from PIL import Image #Python Imaging Library is an open-source library that provides support for opening, manipulating, and saving many different image file formats.
import plotly.express as px #Plotly Express provides more than 30 functions for creating different types of figures.
import pandas_profiling #pandas_profiling library in Python include a method named as ProfileReport() which generate a basic report on the input DataFrame. 
from pathlib import Path
import logging
import re
import random
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.corpus import wordnet
import warnings
import sklearn
from sklearn import metrics
import statistics # for calculating mode of the data
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix
from simpletransformers.classification import ClassificationModel
from streamlit_pandas_profiling import st_profile_report
from keras.models import load_model
from wordcloud import WordCloud
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Subtract, LSTM, Embedding, Bidirectional
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.initializers import Constant
from tensorflow.keras.models import Sequential, Model
import torch  #The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.
              #It has a CUDA counterpart, that enables you to run your tensor computations on GPU 
import torch.nn as nn #Pytorch classes that helps to create and train Neural Networks. 
from nlp import Dataset # Dataset stores the samples and their corresponding labels
from torch.utils.data import DataLoader #DataLoader wraps an iterable with specified batch size around the Dataset.
from sklearn import datasets #scikit-learn comes with a few small standard datasets that do not require to download any file from some external website
from datasets import load_metric # Used to load the metrics for calculating metrics
from transformers import BertTokenizerFast, AutoModelForSequenceClassification
#AutoModelForSequenceClassification model class will be instantiated as one of the model classes of the library (with a sequence classification head)

from transformers import RobertaTokenizer,RobertaForSequenceClassification #RobertaConfig, RobertaModel, 
pd.set_option('max_colwidth',200)
pd.set_option('max_columns',200)
import random
random.seed(100)
SEED = 100


PAGE_CONFIG = {"page_title":"StColab.io","page_icon":":smiley:","layout":"centered"}
st.set_page_config(**PAGE_CONFIG)
warnings.filterwarnings("ignore", category=DeprecationWarning)
st.set_option('deprecation.showPyplotGlobalUse', False)

#cover_image = Image.open('/content/drive/MyDrive/Colab_Notebooks/Capstone Project/cover.png')
#st.image(cover_image, use_column_width=True)  

class DataFrame_Loader():    
    def __init__(self):        
        print("Loading the DataFrame file....")
        
    def read_csv(self,data):
        self.df = pd.read_csv(data)
        return self.df

    def save_df1(self,data,filename):
         path = "/content/drive/MyDrive/Colab Notebooks/Capstone Project/Team Files"
         filename = path+filename
         DF = pd.DataFrame(data)
         DF.to_csv(filename, encoding = 'utf-8', index=False)
         st.write("Saved at:",filename)

class EDA():
    def __init__(self):
        print("Performing Exploratory Data Analysis")

    def dis_shape(self,x):
        return x.shape

    def dis_dtypes(self,x):
      import pyarrow as pa
      df_t = pd.DataFrame(columns=['Name','dtypes'])
      string = pa.Schema.from_pandas(x).to_string()
      for str1 in string.split('\n'):
        str2 = str1.split(':',1)
        if len(str2) == 2 :
          df_t.loc[len(df_t.index)] = [str2[0],str2[1]]
      df_t.drop(df_t.tail(2).index,inplace=True)
      return df_t

    def dis_missing(self,x):
    	  return x.isna().sum()

    def tabulation(self,x):
        table=pd.DataFrame(columns=['Columns','No. of missing values','% of missing values','No. of unique values','No. of non-zero values'])
        # table = self.dis_dtypes(x)
        table['Columns'] = x.columns
        table['No. of missing values'] = x.isnull().sum().values
        table['% of missing values'] = ((x.isnull().sum().values)/ (x.shape[0])) *100
        table['No. of unique values'] = x.nunique().values
        table['No. of non-zero values'] = np.count_nonzero(x, axis=0)
        return table

    def dis_five_number_summary(self,x):
        return x.describe().transpose()

    def dis_columns(self,x):
        return x.columns

    def dis_categorical_variables(self,x):
        cat_var = [var for var in x.columns if x[var].dtypes=="object"]
        cat_var = x[cat_var]
        return cat_var

    def dis_numerical_variables(self,x):
        num_var = [var for var in x.columns if x[var].dtypes!="object"]
        num_var = x[num_var]
        return num_var

    def dis_duplicates(self,x,*args):
        duplicateRowsDF = x[x.duplicated()]
        return (duplicateRowsDF)

class visualize_data():
    def __init__(self):
        print("Visualizing data")

    def vis_displot(self,x):
        plt.style.use('fivethirtyeight')
        plt.figure(figsize=(20,10))
        return sns.distplot(x, bins = 25)

    def vis_countplot(self,x,hue):
        plt.style.use('fivethirtyeight')
        plt.figure(figsize=(20,10))
        return sns.countplot(x, hue=hue, palette='summer')

    def vis_barplot(self,x,y):
        plt.figure(figsize=(20,10))
        plt.xticks(rotation=90)
        plt.tight_layout()
        return sns.barplot(x, y, palette="ch:.25")

    def vis_piechart(self,x,col):
        colors = ['tab:blue', 'tab:cyan', 'tab:gray', 'tab:orange', 'tab:red']
        plt.figure(figsize=(10,5))
        return x[col].value_counts().plot(kind='pie', colors = colors, autopct='%.0f%%', shadow = True)

    def vis_boxplot(self,x):
        plt.figure(figsize=(20,5))
        return sns.boxplot(x=x, palette='summer')

    def vis_histplot(self,x):
    	return x.hist()
     
    def vis_pairplot(self,x):
        return sns.pairplot(x, palette='spring')

    def vis_heatmap(self,x):
        plt.figure(figsize=(20,20))
        return sns.heatmap(x.corr(),annot=True,cmap="YlGnBu");

    def vis_scatterplot(self,x,col1,col2):
        rng = np.random.RandomState(0)
        colors = rng.rand(x.shape[0])
        return plt.scatter(col1,col2,c=colors,cmap='viridis')      

    def vis_fulldisplot(self,x):
        color = sns.color_palette("viridis", 20)
        fig, ax = plt.subplots(figsize = (20, 65))
        for n, col in enumerate(x.columns):
          try:
            if (x[col].dtype.name == 'int64' or x[col].dtype.name == 'float64'):
              if x.shape[1]%2==0:
                plt.subplot((x.shape[1]+1)/2, 2, n)
                sns.distplot(x[col], kde = True, color=color[n-3])
                plt.title(f"\nDistribution of {col}\n",fontdict=dict(fontsize=20))
                plt.tight_layout();
              else:
                plt.subplot((x.shape[1]+1)/2, 2, n+1)
                sns.distplot(x[col], kde = True, color=color[n-3])
                plt.title(f"\nDistribution of {col}\n",fontdict=dict(fontsize=20))
                plt.tight_layout();               
            else:
              print(f"\n Distplot is unavailable for categorical variable '{col}'")
              pass 
          except IndexError:
              pass

    def vis_fullboxplot(self,x):
        color = sns.cubehelix_palette(20)
        fig, ax = plt.subplots(figsize = (20, 65))
        for n, col in enumerate(x.columns):
          try:
            if (x[col].dtype.name == 'int64' or x[col].dtype.name == 'float64'):
              if x.shape[1]%2==0:
                plt.subplot((x.shape[1]+1)/2, 2, n)
                sns.boxplot(x[col], color=color[n-3])
                plt.title(f"\nDistribution of {col}\n",fontdict=dict(fontsize=20))
                plt.tight_layout();
              else:
                plt.subplot((x.shape[1]+1)/2, 2, n+1)
                sns.boxplot(x[col], color=color[n-3])
                plt.title(f"\nDistribution of {col}\n",fontdict=dict(fontsize=20))
                plt.tight_layout();               
            else:
              print(f"\n Boxplot is unavailable for categorical variable '{col}'")
              pass 
          except IndexError:
              pass

    def vis_wordcloud(self,x):
	    wordcloud = WordCloud(width = 3000, height = 2000).generate(" ".join(x))
	    plt.imshow(wordcloud);
	    plt.axis("off")
	    return wordcloud  

class preprocess_data():
    def __init__(self):
        print("Data Preprocessing : ")

    def drop_Column(self,x,col_name):
      st.write(col_name)
      return x.drop(columns=col_name)

    def drop_rows_withoutApt_data(self,x):
      return x.dropna(axis=0)
    
    def remove_stopword(self,x):
      import nltk #NLP Tool Kit
      from nltk.corpus import stopwords
      nltk.download('stopwords')
      stopwords=set(stopwords.words('english'))
      return x.apply(lambda s: ' '.join([words for words in s.split() if words not in stopwords]))
    
    def tokenize(self,x):
      from keras.preprocessing.text import Tokenizer
      vocab_size = 10000
      oov_token = "<OOV>"
      tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)
      tokenizer.fit_on_texts(x)
      return tokenizer.texts_to_sequences(x)

    def select_feature(self,x,columnName):
      return x[columnName]
    
    def select_label(self,y,columnName):
      return y[columnName]
    
    def padding(self,x,length,paddingType,truncationType):
      from keras.preprocessing.sequence import pad_sequences
      return pad_sequences(x,length,padding=paddingType,truncating=truncationType)

    def label_classification(self,x):
      from sklearn.preprocessing import OneHotEncoder
      oneHotEnc = OneHotEncoder()
      return oneHotEnc.fit_transform(x.values.reshape(-1,1)).toarray()
    
    def train_test_split(self,feature,label):
      from sklearn.model_selection import train_test_split
      X_train, X_test, y_train, y_test = train_test_split(feature, label, test_size=0.3, random_state=42)
      X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3, random_state=42)
      return X_train,y_train,X_test,y_test,X_val,y_val

    def print_Len_details(self,x):
      X_sequences = x
      lenData = len(X_sequences)
      maxWordCount = 0
      minWordCount = len(X_sequences[0])
      avgWordCount = 0
      totalWordCount = 0
      wordCount_list = []
      for i in range(lenData):
        wordCount = len(X_sequences[i])
        wordCount_list.append(wordCount)
        avgWordCount = (wordCount+totalWordCount) / (i+1)
        totalWordCount = totalWordCount + wordCount
        if wordCount > maxWordCount :
            maxWordCount = wordCount
        
        if wordCount < minWordCount :
            minWordCount = wordCount

      st.write("Maximum Word Limit in a Review : ",maxWordCount)
      st.write("Minimum Word Limit in a Review : ",minWordCount)
      st.write("Average Word Limit in a Review across all Reviews : ",avgWordCount)

class models():
    def __init__(self):
        print("Building models")

    def fsl_1inp(self, df_train, df_test):
        labels = df_train['Accident Level'].unique()
        text_left = []
        text_right = []
        target = []

        for label in labels:
            
            similar_texts = df_train[df_train['Accident Level']==label]['cleaned_text']
            group_similar_texts = list(itertools.combinations(similar_texts,2))
            
            text_left.extend([group[0] for group in group_similar_texts])
            text_right.extend([group[1] for group in group_similar_texts])
            target.extend([1.]*len(group_similar_texts))

            dissimilar_texts = df_train[df_train['Accident Level']!=label]['cleaned_text']
            for i in range(len(group_similar_texts)):
                text_left.append(np.random.choice(similar_texts))
                text_right.append(np.random.choice(dissimilar_texts))
                target.append(0.)
                
        dataset = pd.DataFrame({'text_left':text_left,
                            'text_right':text_right,
                            'target': target})
        st.write('The Siamese Dataframe is created.')
        st.dataframe(dataset.head())
        MAX_SEQ_LENGTH = 200
        VOCAB_SIZE = 10000

        tokenizer = Tokenizer(num_words=VOCAB_SIZE)
        tokenizer.fit_on_texts(df_train.cleaned_text)
        sequences_left = tokenizer.texts_to_sequences(dataset.text_left)
        sequences_right = tokenizer.texts_to_sequences(dataset.text_right)

        word_index = tokenizer.word_index

        x_left = pad_sequences(sequences_left, maxlen=MAX_SEQ_LENGTH)
        x_right = pad_sequences(sequences_right, maxlen=MAX_SEQ_LENGTH)


        # load model in Keras
        siamese_lstm = load_model('/content/drive/MyDrive/Colab Notebooks/Capstone Project/Models/fsl_model_1input.h5')

        reference_sequences = tokenizer.texts_to_sequences(df_train.cleaned_text)
        x_reference_sequences = pad_sequences(reference_sequences, maxlen=MAX_SEQ_LENGTH)

        def flatten_text_sequence(text):
            flatten = itertools.chain.from_iterable
            text = list(flatten(text))
            return text

        def get_prediction(text):
            """ Get the predicted category, and the most similar text
            in the train set. Note that this way of computing a prediction is highly 
            not optimal, but it'll be sufficient for us now. """
            x = tokenizer.texts_to_sequences(text.split())
            x = flatten_text_sequence(x)
            x = pad_sequences([x], maxlen=MAX_SEQ_LENGTH)

            # Compute similarities of the text with all text's in the train set
            result = np.repeat(x, len(x_reference_sequences), axis=0)
            similarities = siamese_lstm.predict([result, x_reference_sequences])
            most_similar_index = np.argmax(similarities)
            
            # The predicted category is the one of the most similar example from the train set
            prediction = df_train['Accident Level'].iloc[most_similar_index]
            most_similar_example = df_train['cleaned_text'].iloc[most_similar_index]

            return prediction, most_similar_example

        classes_encoder = LabelEncoder()

        y_train = classes_encoder.fit_transform(df_train['Accident Level'])
        y_test = classes_encoder.transform(df_test['Accident Level'])

        y_pred = [get_prediction(Description)[0] for Description in df_test['Description']]
        accuracy = accuracy_score(classes_encoder.transform(y_pred), y_test)

        target_names = ['Acc Level 1', 'Acc Level 2', 'Acc Level 3', 'Acc Level 4', 'Acc Level 5']
        cm = confusion_matrix(classes_encoder.transform(y_pred), y_test)
        # print(classification_report(classes_encoder.transform(y_pred), y_test, target_names=target_names))

        def plot_confusion_matrix(cm, classes,
                                normalize=False,
                                title='Confusion matrix',
                                cmap=plt.cm.Purples):
            """
            This function prints and plots the confusion matrix.
            Normalization can be applied by setting `normalize=True`.
            """
            plt.imshow(cm, interpolation='nearest', cmap=cmap)
            plt.title(title)
            plt.colorbar()
            tick_marks = np.arange(len(classes))
            plt.xticks(tick_marks, classes, rotation=45)
            plt.yticks(tick_marks, classes)

            if normalize:
                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
                print("Normalized confusion matrix")
            else:
                print('Confusion matrix, without normalization')

            thresh = cm.max() / 2.
            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
                plt.text(j, i, cm[i, j],
                    horizontalalignment="center",
                    color="white" if cm[i, j] > thresh else "black")

            plt.tight_layout()
            plt.ylabel('True label')
            plt.xlabel('Predicted label')

        precision = metrics.precision_score(y_test, classes_encoder.transform(y_pred),average='macro')
        recall = metrics.recall_score(y_test, classes_encoder.transform(y_pred), average='micro')
        f1 = metrics.f1_score(y_test, classes_encoder.transform(y_pred), average='weighted')
        model_performance = pd.DataFrame(columns=['Model', 'Accuracy', 'Log Loss','Precision', 'Recall', 'F1'])
        model_performance = model_performance.append({'Model':'FSL + BiLSTM + Data Aug (Syn replacement) for class II, III, IV and V',
                                      'Accuracy': accuracy,
                                      'Log Loss': 'NA',
                                      'Precision': precision,
                                      'Recall': recall,
                                      'F1': f1                                    
                                      }, ignore_index=True)


        return (f'Test accuracy: {100*accuracy:.2f} %', 
                plot_confusion_matrix(cm=cm, classes=target_names, title='Confusion Matrix for Few-Shot Learning Model with 1 feature'),
                classification_report(classes_encoder.transform(y_pred), y_test, target_names=target_names),
                model_performance
                )

    def xlnet(self,df_test):
        model_type = 'xlnet'
        model_name = 'xlnet-base-cased'

        model = ClassificationModel(model_type, "/content/drive/MyDrive/Colab Notebooks/Capstone Project/outputs/xlnet/final", use_cuda=True,)
        y_preds, _, = model.predict(df_test['text'])


        def compute_metrics(y_true, y_pred):
            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')
            return {
                'f1': f1,
                'precision': precision,
                'recall': recall
            }

        # Evaluate the model
        result, model_outputs, wrong_predictions = model.eval_model(df_test, 
                                                                    acc = sklearn.metrics.accuracy_score
                                                                    )
        
        def plot_cm(y_true, y_pred, title, figsize=(5,5)):
            cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))
            cm_sum = np.sum(cm, axis=1, keepdims=True)
            cm_perc = cm / cm_sum.astype(float) * 100
            annot = np.empty_like(cm).astype(str)
            nrows, ncols = cm.shape
            for i in range(nrows):
                for j in range(ncols):
                    c = cm[i, j]
                    p = cm_perc[i, j]
                    if i == j:
                        s = cm_sum[i]
                        annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
                    elif c == 0:
                        annot[i, j] = ''
                    else:
                        annot[i, j] = '%.1f%%\n%d' % (p, c)
            cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))
            cm.index.name = 'Actual'
            cm.columns.name = 'Predicted'
            fig, ax = plt.subplots(figsize=figsize)
            plt.title(title)
            sns.heatmap(cm, cmap= "YlGnBu", annot=annot, fmt='', ax=ax)
        
        return (result, 
                plot_cm(y_preds, df_test['label'].values, 'Confusion matrix for XLNet model', figsize=(7,7)),
                compute_metrics(df_test['label'], y_preds))

    def distilbert(self,df_test):
        model_type = 'distilbert'
        model_name = 'distilbert-base-cased'

        model = ClassificationModel(model_type, "/content/drive/MyDrive/Colab Notebooks/Capstone Project/outputs/distilbert/final", use_cuda=True,)

        def compute_metrics(y_true, y_pred):
            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')
            return {
                'f1': f1,
                'precision': precision,
                'recall': recall
            }	

        # Evaluate the model
        result, model_outputs, wrong_predictions = model.eval_model(df_test, 
                                                                    acc = sklearn.metrics.accuracy_score
                                                                    )
        
        def plot_cm(y_true, y_pred, title, figsize=(5,5)):
            cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))
            cm_sum = np.sum(cm, axis=1, keepdims=True)
            cm_perc = cm / cm_sum.astype(float) * 100
            annot = np.empty_like(cm).astype(str)
            nrows, ncols = cm.shape
            for i in range(nrows):
                for j in range(ncols):
                    c = cm[i, j]
                    p = cm_perc[i, j]
                    if i == j:
                        s = cm_sum[i]
                        annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
                    elif c == 0:
                        annot[i, j] = ''
                    else:
                        annot[i, j] = '%.1f%%\n%d' % (p, c)
            cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))
            cm.index.name = 'Actual'
            cm.columns.name = 'Predicted'
            fig, ax = plt.subplots(figsize=figsize)
            plt.title(title)
            sns.heatmap(cm, cmap= "YlGnBu", annot=annot, fmt='', ax=ax)

        y_preds, _, = model.predict(df_test['text'])

        
        return (result, 
                plot_cm(y_preds, df_test['label'].values, 'Confusion matrix for DistilBert model', figsize=(7,7)),
                compute_metrics(df_test['label'], y_preds))

class modeldisplay_functions():
    def __init__(self):        
        print("Loading BERT models")
        self.__bert_df__ = pd.DataFrame()
        

    def data_preprocess01(self,df): # Function to create a DataFrame with 2 columns "Accident Level" and "Description"
      self.__bert_df__ = df     
      self.__bert_df__ = self.__bert_df__.drop(['Unnamed: 0','Data','Countries','Local','Industry Sector','Potential Accident Level','Genre','Employee or Third Party','Critical Risk'], axis = 1) # Drop of Attributes
      categorical_to_numeric = {"Accident Level": {"I": 0, "II": 1,"III": 2, "IV": 3,"V": 4}}
      self.__bert_df__ = self.__bert_df__.replace(categorical_to_numeric)
      return self.__bert_df__ 
    
    def bert_preprocess(self,df):   # function converts the text to tokens and adds padding to the tokens of maximum length 200
      self.bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)
      return self.bert_tokenizer(df['text'], truncation=True, max_length = 200, padding='max_length')

    def roberta_preprocess(self,df):   # function converts the text to tokens and adds padding to the tokens of maximum length 200
      self.roberta_tokenizer = RobertaTokenizer.from_pretrained("roberta-base",do_lower_case=True)
      return self.roberta_tokenizer(df['text'], truncation=True, max_length = 200, padding='max_length')


    def bert_prediction_display(self,text, model): 
      text_list = []
      dataset = {}
      pred_list = []
      prediction = 6

      pred_length = 3 # creating same 'text' 3 times for batch purpose. This can be varied the minimum is 2. 
      for i in range(pred_length): 
        text_list.append(text)  

      dataset = dict([('text', text_list)]) # converts the data to dict format
      dataset = Dataset.from_dict(dataset) # converts the data to Dataset format
      dataset_batch = dataset.map(self.bert_preprocess, batched=True,batch_size=len(dataset)) #converts the text to tokens
      dataset_batch.set_format('torch', columns=['input_ids', 'attention_mask'])
      dataset_dataloader = DataLoader(dataset_batch, shuffle=True, batch_size=64) #converts the data into DataLoader format with a batch size of 64

      dev = torch.device("cpu") #if torch.cuda.is_available() else torch.device("cpu")
      device_pickle = torch.device(dev) # functions that utilize GPU's for computation purpose. 
      model.to(device_pickle)

      model.eval()
      for batch in dataset_dataloader:
        batch = {k: v.to(device_pickle) for k, v in batch.items()}
        with torch.no_grad():
          outputs = model(**batch)

      logits = outputs.logits
    
      predictions = torch.argmax(logits, dim=-1)
      pred_list = predictions.tolist()
      try:
        prediction = statistics.mode(pred_list)
      except:
        prediction = pred_list[0]
      #print("Predictions:{}".format(prediction))
      return prediction

    def roberta_prediction_display(self,text,model): 
      text_list = []
      dataset = {}
      pred_list = []
      prediction = 6

      pred_length = 3 # creating same 'text' 3 times for batch purpose. This can be varied the minimum is 2. 
      for i in range(pred_length): 
        text_list.append(text)  

      dataset = dict([('text', text_list)]) # converts the data to dict format
      dataset = Dataset.from_dict(dataset) # converts the data to Dataset format
      dataset_batch = dataset.map(self.bert_preprocess, batched=True,batch_size=len(dataset)) #converts the text to tokens
      dataset_batch.set_format('torch', columns=['input_ids', 'attention_mask'])
      dataset_dataloader = DataLoader(dataset_batch, shuffle=True, batch_size=64) #converts the data into DataLoader format with a batch size of 64

      dev = torch.device("cpu") #if torch.cuda.is_available() else torch.device("cpu")
      device_roberta = torch.device(dev) # functions that utilize GPU's for computation purpose. 
      model.to(device_roberta)

      model.eval()
      for batch in dataset_dataloader:
        batch = {k: v.to(device_roberta) for k, v in batch.items()}
        with torch.no_grad():
          outputs = model(**batch)

      logits = outputs.logits
    
      predictions = torch.argmax(logits, dim=-1)
      pred_list = predictions.tolist()
      try:
        prediction = statistics.mode(pred_list)
      except:
        prediction = pred_list[0]
      #print("Predictions:{}".format(prediction))
      return prediction         

def main():
  st.title("NLP Chatbot Interface")
  st.info("Capstone Project_Group 11_NLP 2_August 20 Batch")
  
  activities = ["Perform Exploratory Data Analysis","Visualize Data","Preprocess Data","Build Models for NLP Tasks"]
  task = st.sidebar.selectbox("Please select your preferred task",activities)
  
  isDataLoadingReq = True

  if task == 'Perform Exploratory Data Analysis':
    st.subheader("Exploratory Data Analysis")
    data = st.file_uploader("Please upload a Dataset", type=["csv"])
    if data is not None:
      df = load.read_csv(data)
      df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
      st.dataframe(df.head())
      st.success("The dataset is loaded successfully!")

      if st.checkbox("Display the shape of the dataset"):
          st.write(dataframe.dis_shape(df))

      if st.checkbox("Display the object type of attributes"):
          st.write(dataframe.dis_dtypes(df))

      if st.checkbox("Display missing entries in the dataset"):
          st.write(dataframe.dis_missing(df))

      if st.checkbox("Display missing values; unique values; % of missing values; first, second and third observations; and entropy"):
          st.write(dataframe.tabulation(df))

      if st.checkbox("Display five-number summary of numerical attributes"):
          st.write(dataframe.dis_five_number_summary(df))

      if st.checkbox("Display categorical variables"):
          new_df = dataframe.dis_categorical_variables(df)
          catego_df = pd.DataFrame(new_df)
          st.dataframe(catego_df)

      if st.checkbox("Display numerical variables"):
          num_df = dataframe.dis_numerical_variables(df)
          numer_df=pd.DataFrame(num_df)
          st.dataframe(numer_df)

      if st.checkbox("Display value counts of categories in categorical variables"):
          cat_vars = [var for var in df.columns if df[var].dtypes=="object"]
          sel_cat_var = st.selectbox("Select the categorical variable",cat_vars)
          # col = pd.DataFrame(df[sel_cat_var].values)
          st.write(df[sel_cat_var].value_counts())

      if st.checkbox("Display skewness of the dataset"):
          st.write(df.skew(axis = 0, skipna = True))

      if st.checkbox("Display duplicate rows in the dataset"):
          all_cols = dataframe.dis_columns(df)
          data1 = dataframe.dis_duplicates(df)
          st.write(data1)
          st.write("The number of duplicates are: ", len(data1))

  elif task == 'Visualize Data':
    st.subheader("Visualizing the distribution of data")
    data = st.file_uploader("Please upload a Dataset", type=["csv"])
    if data is not None:
      df = load.read_csv(data)
      df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
      st.dataframe(df.head())
      st.success("The dataset is loaded successfully!")

      st.subheader('Univariate Analysis')

      if st.checkbox("Display DISTPLOT of numerical attributes"):
          num_vars = [var for var in df.columns if df[var].dtypes!="object"]
          sel_num_var = st.selectbox("Select the numerical variable",num_vars, key="num_vars1")
          if st.button("Generate distplot for the selected variable"):
            st.write(visualize.vis_displot(df[sel_num_var]))
            st.pyplot()

      if st.checkbox("Display COUNTPLOT of categorical attributes"):
          cat_vars = [var for var in df.columns if df[var].dtypes=="object"]
          sel_cat_var = st.selectbox("Select the categorical variable",cat_vars)
          hue = st.selectbox("Select the hue attribute",cat_vars, key="cat_vars1")
          if st.button("Generate countplot for the selected variable"):
            st.write(visualize.vis_countplot(df[sel_cat_var],df[hue]))
            st.pyplot()

      if st.checkbox("Display PIECHART of categorical attributes"):
          cat_vars = [var for var in df.columns if df[var].dtypes=="object"]
          sel_cat_var1 = st.selectbox("Select the categorical variable",cat_vars, key="cat_vars2")
          if st.button("Generate piechart for the selected variable"):
            st.write(visualize.vis_piechart(df,sel_cat_var1))
            st.pyplot()

      if st.checkbox("Display outliers using BOXPLOTS"):
          cols = dataframe.dis_columns(df) 
          sel_x = st.selectbox("Select the variable",cols, key="cols3")
          if st.button("Generate boxplot for the selected variable"):
            st.write(visualize.vis_boxplot(df[sel_x]))
            st.pyplot()
      
      if st.checkbox("Display HISTOGRAM of attributes"):
          cols = dataframe.dis_columns(df) 
          sel_col = st.selectbox("Select the variable",cols, key="cols4")
          if st.button("Generate histogram for the selected variable"):
            st.write(visualize.vis_histplot(df[sel_col]))
            st.pyplot()

      st.subheader('Bivariate Analysis')

      if st.checkbox("Display SCATTERPLOT of numerical attributes"):
          num_vars = [var for var in df.columns if df[var].dtypes!="object"]
          sel_num_var1 = st.selectbox("Select the first numerical variable",num_vars, key="num_vars2")
          sel_num_var2 = st.selectbox("Select the second numerical variable",num_vars, key="num_vars3")
          if st.button("Generate scatterplot for the selected variables"):
            st.write(visualize.vis_scatterplot(df,df[sel_num_var1],df[sel_num_var2]))
            st.pyplot()

      if st.checkbox("Display BARPLOT  of attributes"):
          cols = dataframe.dis_columns(df) 
          sel_x = st.selectbox("Select the x variable",cols, key="cols1")
          sel_y = st.selectbox("Select the y variable",cols, key="cols2")
          if st.button("Generate barplot for the selected variables"):
            st.write(visualize.vis_barplot(df[sel_x],df[sel_y]))
            st.pyplot()

      st.subheader("Multivariate Analysis")

      if st.checkbox("Display PAIRPLOT of numerical attributes"):
          st.write(visualize.vis_pairplot(df))
          st.pyplot()

      if st.checkbox("Display HEATMAP to view the correlation between numerical attributes"):
          st.write(visualize.vis_heatmap(df))
          st.pyplot()

      if st.checkbox("Display DISTPLOT of the entire dataset"):
          st.write(visualize.vis_fulldisplot(df))
          st.pyplot()

      if st.checkbox("Display BOXPLOT of the entire dataset"):
          st.write(visualize.vis_fullboxplot(df))
          st.pyplot()

      # if st.checkbox("Display the complete data PROFILE REPORT"):
      #     pr = df.profile_report()
      #     st_profile_report(pr)

      if st.checkbox("Display WORD CLOUD"):
          cols = dataframe.dis_columns(df) 
          sel_col = st.selectbox("Select the attribute",cols, key="cols10")
          if st.button("Generate word cloud for the selected attribute"):        
            st.write(visualize.vis_wordcloud(df[sel_col]))
            st.pyplot()

  elif task == 'Preprocess Data':
    st.subheader("Pre-processing data to get clean data for modelling")
    data = st.file_uploader("Please upload a Dataset", type=["csv"])
    if data is not None:
      if isDataLoadingReq == True :
        df = load.read_csv(data)
        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
        df_EDA = df.copy(deep = True)
        st.write("Reloading data !!!!! ")
        isDataLoadingReq = False
        st.subheader("Pre-processing based on EDA Analysis")
        st.write("Dropping the unnecessary columns")
        column_name = [var for var in df.columns]
        column_to_keep = ['Accident Level', 'Description']
        column_to_drop = ['Data', 'Countries', 'Local', 'Industry Sector', \
                          'Potential Accident Level', 'Genre', \
                          'Employee or Third Party', 'Critical Risk']
        st.write("Based on EDA, we will drop the following Columns")
        st.write(column_to_drop)
        st.write("Based on EDA, we will retain the following Columns")
        st.write(column_to_keep)
        df_EDA = df_EDA.drop(columns=column_to_drop)
        st.write(df_EDA)
        st.write(dataframe.dis_shape(df_EDA))

        st.subheader("Selecting Features and Labels for Model Input")
        features = df_EDA['Description']
        labels = df_EDA['Accident Level']
        st.write("Features",features)
        st.write("Labels",labels)

        st.subheader("Removing Stopwords: ")
        features = preprocess.remove_stopword(features)
        st.write("Features after StopWord Removal",features)
        
        st.subheader("Tokenizing:")
        features = preprocess.tokenize(features)
        preprocess.print_Len_details(features)
        st.write("Features after Tokenization",features)

        st.subheader("Padding Sequences:")
        max_length = 200
        padding_type='post'
        truncation_type='post'  
        features = preprocess.padding(features,max_length,padding_type,truncation_type)
        preprocess.print_Len_details(features)
        st.write("Features after Padding",features)

        st.subheader("Label Class Identification:")
        labels_enc = preprocess.label_classification(labels)
        st.write(labels_enc)

        st.subheader("Splitting data for Model : train / validation / test split")
        X_train,y_train,X_test,y_test,X_val,y_val = preprocess.train_test_split(features,labels_enc)
        st.write("Training data shape = ",X_train.shape)
        st.write("Test data shape = ",X_test.shape)
        st.write("Validation data shape = ",X_val.shape)

        st.subheader("Saving file for the Model:")
        load.save_df1(X_train,"X_train.csv")
        load.save_df1(y_train,"y_train.csv")
        load.save_df1(X_test,'X_test.csv')
        load.save_df1(y_test,'y_test.csv')
        load.save_df1(X_val,'X_val.csv')
        load.save_df1(y_val,'y_val.csv')

  elif task == 'Build Models for NLP Tasks':
    st.subheader("Building Models for NLP Tasks")
    data = st.file_uploader("Please upload a Dataset", type=["csv"])
    if data is not None:
      df = load.read_csv(data)
      st.dataframe(df.head())
      st.success("The dataset is loaded successfully!")

    if st.checkbox("Display of preprocessed data"):
        #df = load.read_csv(data)
        preprocess_df = bm_func.data_preprocess01(df)
        st.write(preprocess_df.head())

    st.subheader('Please select a model:')

    if st.checkbox("XLNet Model"):
        # data_train_xl = st.file_uploader("Please upload the Dataset", type=["csv"], key="k0")
        # if data_train_xl is not None:
        #   df_train_xl = load.read_csv(data_train_xl)
        #   st.dataframe(df_train_xl.head())
        #   st.success("The dataset is loaded successfully!")
        data_test_xl = st.file_uploader("Please upload the Test Dataset", type=["csv"], key="k1")
        if data_test_xl is not None:
          df_test_xl = load.read_csv(data_test_xl)
          st.dataframe(df_test_xl.head())
          st.success("The dataset is loaded successfully!")
          st.write(models.xlnet(df_test_xl))
          st.pyplot()  

    if st.checkbox("DistilBert Model"):
        # data_train_db = st.file_uploader("Please upload the Dataset", type=["csv"], key="k2")
        # if data_train_db is not None:
        #   df_train_db = load.read_csv(data_train_db)
        #   st.dataframe(df_train_db.head())
        #   st.success("The dataset is loaded successfully!")
        data_test_db = st.file_uploader("Please upload the Test Dataset", type=["csv"], key="k3")
        if data_test_db is not None:
          df_test_db = load.read_csv(data_test_db)
          st.dataframe(df_test_db.head())
          st.success("The dataset is loaded successfully!")
          st.write(models.distilbert(df_test_db))
          st.pyplot()     
        
    if st.checkbox("BERT Model Prediction"):
      with st.form(key='my_form1'):
        text_input1 = st.text_area("Please enter the accident description within 200 characters:",max_chars = 500)
        submit_button1 = st.form_submit_button(label='Predict the Accident Level')
      
      if submit_button1:          
        #tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)  
        pickledmodel_bert = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)
        pickledmodel_bert.load_state_dict(torch.load("/content/drive/MyDrive/Colab Notebooks/Capstone Project/Team Files/Srinath/model_BERT.pth"))
        #st.write(torch.cuda.is_available())
        
        prediction_bert = bm_func.bert_prediction_display(text_input1,pickledmodel_bert) 
        AccidentLevel = ['I','II','III','IV','V'] # to display the text
        st.success("Predicted Accident Level based on the Description:")
        st.write(AccidentLevel[prediction_bert])  

    if st.checkbox("RoBERTa Model Prediction"):
      with st.form(key='my_form2'):
        text_input2 = st.text_area("Please enter the accident description within 200 characters:",max_chars = 500)
        submit_button2 = st.form_submit_button(label='Predict the Accident Level')
      
      if submit_button2:          
        
        pickledmodel_Roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)
        pickledmodel_Roberta.load_state_dict(torch.load("/content/drive/MyDrive/Colab Notebooks/Capstone Project/Team Files/Srinath/model_Roberta1.pth"))
        #st.write(torch.cuda.is_available())
        
        prediction_roberta = bm_func.roberta_prediction_display(text_input2,pickledmodel_Roberta) 
        AccidentLevel = ['I','II','III','IV','V'] # to display the text
        st.success("Predicted Accident Level based on the Description:")
        st.write(AccidentLevel[prediction_roberta])   


    if st.checkbox("Few-Shot Learning Model with BiLSTM"):
        data_train_fsl = st.file_uploader("Please upload the Train Dataset to create the Siamese Dataframe", type=["csv"], key="k4")
        if data_train_fsl is not None:
          df_train_fsl = load.read_csv(data_train_fsl)
          st.dataframe(df_train_fsl.head())
          st.success("The dataset is loaded successfully!")
        data_test_fsl = st.file_uploader("Please upload the Test Dataset", type=["csv"], key="k5")
        if data_test_fsl is not None:
          df_test_fsl = load.read_csv(data_test_fsl)
          st.dataframe(df_test_fsl.head())
          st.success("The dataset is loaded successfully!")
          st.write(models.fsl_1inp(df_train_fsl, df_test_fsl))
          st.pyplot()



      
if __name__ == '__main__':
    load = DataFrame_Loader()
    dataframe = EDA()
    visualize = visualize_data()
    preprocess = preprocess_data()
    bm_func = modeldisplay_functions()
    models = models()
    main()

